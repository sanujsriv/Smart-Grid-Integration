{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/grad16/sakumar/sparkNMSU\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/dbr1/hctl/spark_essentials/jdk1.8.0_251/\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/local/spark-2.4.0-bin-hadoop2.7/\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark_iforest.ml.iforest import *\n",
    "from decimal import Decimal\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import to_timestamp,to_date\n",
    "from datetime import datetime\n",
    "import time as realtime\n",
    "from pyspark.sql import SQLContext,SparkSession\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"IForestExample\").config(\"spark.jars\", \"/dbr1/hctl/spark_essentials/mysql-connector-java-5.1.49.jar,/dbr1/hctl/spark_essentials/spark-iforest-2.4.0.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ggplot style for more sophisticated visuals\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def live_plotter(x_vec,y1_data,line1,identifier='',pause_time=0.1):\n",
    "    if line1==[]:\n",
    "        # this is the call to matplotlib that allows dynamic plotting\n",
    "        #plt.ion()\n",
    "        fig = plt.figure(figsize=(13,6))\n",
    "        ax = fig.add_subplot(111)\n",
    "        # create a variable for the line so we can later update it\n",
    "        line1, = ax.plot(x_vec,y1_data,'-',alpha=0.8)        \n",
    "        #update plot label/title\n",
    "        plt.ylabel('Y Label')\n",
    "        plt.title('Title:  {}'.format(identifier))\n",
    "        #plt.show()\n",
    "    \n",
    "    # after the figure, axis, and line are created, we only need to update the y-data\n",
    "    line1.set_ydata(y1_data)   \n",
    "#     line1.set_xdata(x_vec)\n",
    "       \n",
    "    # adjust limits if new data goes beyond bounds\n",
    "    if np.min(y1_data)<=line1.axes.get_ylim()[0] or np.max(y1_data)>=line1.axes.get_ylim()[1]:\n",
    "        plt.ylim([np.min(y1_data)-np.std(y1_data),np.max(y1_data)+np.std(y1_data)])\n",
    "#     plt.xlim(x_vec[-2],x_vec[-1])\n",
    "#     if np.min(x1_data)<=line1.axes.get_xlim()[0] or np.max(x1_data)>=line1.axes.get_xlim()[1]:\n",
    "#          plt.xlim([np.min(x1_data)-np.std(x1_data),np.max(y1_data)+np.std(y1_data)])\n",
    "    # this pauses the data so the figure/axis can catch up - the amount of pause can be altered above\n",
    "    #plt.pause(pause_time)\n",
    "    \n",
    "    # return line so we can update it again in the next iteration\n",
    "    return line1\n",
    "\n",
    "\n",
    "def live_plotters(x_vec,y1_data,y2_data,y3_data,y4_data,y5_data,y6_data,line1,line2,line3,line4,line5,line6,anomaly_date,pred_list,identifier='',pause_time=0.1):\n",
    "    if line1==[]:\n",
    "       \n",
    "        # this is the call to matplotlib that allows dynamic plotting\n",
    "        #plt.ion()\n",
    "        \n",
    "        fig = plt.figure(figsize=(13,6))\n",
    "        ax = fig.add_subplot(111)\n",
    "        i=0\n",
    "        # create a variable for the line so we can later update it\n",
    "        #line1, = ax.plot(x_vec,y1_data,'-',alpha=0.8) \n",
    "        line2, = ax.plot(x_vec,y2_data,'-',alpha=0.8)\n",
    "        line3, = ax.plot(x_vec,y3_data,'-',alpha=0.8)\n",
    "        line4, = ax.plot(x_vec,y4_data,'-',alpha=0.8)\n",
    "        line5, = ax.plot(x_vec,y5_data,'-',alpha=0.8)\n",
    "        line6, = ax.plot(x_vec,y6_data,'-',alpha=0.8)\n",
    "#         line7, = ax.plot(x_vec,y7_data,'-',alpha=0.8)\n",
    "#         line8, = ax.plot(x_vec,y8_data,'-',alpha=0.8)\n",
    " \n",
    "        #line1.set_label('Score(testing)')\n",
    "        line2.set_label('glp')\n",
    "        line3.set_label('grp')\n",
    "        line4.set_label('sub_metering_1')\n",
    "        line5.set_label('sub_metering_2')\n",
    "        line6.set_label('sub_metering_3')\n",
    "#         line7.set_label('Voltage')\n",
    "#         line8.set_label('Global Index')\n",
    "        \n",
    "        ax.legend()\n",
    "#         ax.set_xlim(left=max(datetime(2006, 12, 21, 4, 6),x_vec[i]), right=x_vec[i+5])\n",
    "        \n",
    "#         ax.scatter(anomaly_date,pred_list)\n",
    "        #update plot label/title\n",
    "        plt.ylabel('Y Label')\n",
    "        plt.title('Title:{}'.format(identifier))\n",
    "        plt.draw()\n",
    "#         plt.show()\n",
    "    # after the figure, axis, and line are created, we only need to update the y-data\n",
    "    \n",
    "    #line1.set_ydata(y1_data)\n",
    "    line2.set_ydata(y2_data)\n",
    "    line3.set_ydata(y3_data)\n",
    "    line4.set_ydata(y4_data)\n",
    "    line5.set_ydata(y5_data)\n",
    "    line6.set_ydata(y6_data)\n",
    "#     line7.set_ydata(y7_data)\n",
    "#     line8.set_ydata(y8_data)\n",
    "    \n",
    "    # adjust limits if new data goes beyond bounds\n",
    "    plt.gca().set_xlim(left=x_vec[0])\n",
    "    #if np.min(y1_data)<=line1.axes.get_ylim()[0] or np.max(y1_data)>=line1.axes.get_ylim()[1]:\n",
    "    #    plt.ylim([np.min(y1_data)-np.std(y1_data),np.max(y1_data)+np.std(y1_data)])\n",
    "    if np.min(y2_data)<=line2.axes.get_ylim()[0] or np.max(y2_data)>=line2.axes.get_ylim()[1]:\n",
    "        plt.ylim([np.min(y2_data)-np.std(y1_data),np.max(y2_data)+np.std(y2_data)])   \n",
    "    if np.min(y3_data)<=line3.axes.get_ylim()[0] or np.max(y3_data)>=line3.axes.get_ylim()[1]:\n",
    "        plt.ylim([np.min(y3_data)-np.std(y1_data),np.max(y3_data)+np.std(y3_data)])\n",
    "    if np.min(y4_data)<=line4.axes.get_ylim()[0] or np.max(y4_data)>=line4.axes.get_ylim()[1]:\n",
    "        plt.ylim([np.min(y4_data)-np.std(y1_data),np.max(y4_data)+np.std(y4_data)])\n",
    "    if np.min(y5_data)<=line5.axes.get_ylim()[0] or np.max(y5_data)>=line5.axes.get_ylim()[1]:\n",
    "        plt.ylim([np.min(y5_data)-np.std(y1_data),np.max(y5_data)+np.std(y5_data)])   \n",
    "    if np.min(y6_data)<=line6.axes.get_ylim()[0] or np.max(y6_data)>=line6.axes.get_ylim()[1]:\n",
    "        plt.ylim([np.min(y6_data)-np.std(y1_data),np.max(y6_data)+np.std(y6_data)])\n",
    "    # this pauses the data so the figure/axis can catch up - the amount of pause can be altered above\n",
    "    #plt.pause(pause_time)\n",
    "    \n",
    "    # return line so we can update it again in the next iteration\n",
    "    return line2,line3,line4,line5,line6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[go.Scatter(x=[0, 1], y=[0, 1])],\n",
    "    layout=go.Layout(\n",
    "        xaxis=dict(range=[0, 5], autorange=False),\n",
    "        yaxis=dict(range=[0, 5], autorange=False),\n",
    "        title=\"Start Title\",\n",
    "        updatemenus=[dict(\n",
    "            type=\"buttons\",\n",
    "            buttons=[dict(label=\"Play\",\n",
    "                          method=\"animate\",\n",
    "                          args=[None])])]\n",
    "    ),\n",
    "    frames=[go.Frame(data=[go.Scatter(x=[1, 2], y=[1, 2])]),\n",
    "            go.Frame(data=[go.Scatter(x=[1, 4], y=[1, 4])]),\n",
    "            go.Frame(data=[go.Scatter(x=[3, 4], y=[3, 4])],\n",
    "                     layout=go.Layout(title_text=\"End Title\"))]\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def anomaly_map(anomaly_date,glp,sub1,sub2,sub3,size):\n",
    "    glp_data_anomaly,sub1_data_anomaly,sub2_data_anomaly,sub3_data_anomaly=[[] for _ in range(4)]\n",
    "    for i in anomaly_date:\n",
    "        for j in range(size):\n",
    "            if(i==x_vec[j]):\n",
    "                glp_data_anomaly.append(glp[j])\n",
    "                sub1_data_anomaly.append(sub1[j]) \n",
    "                sub2_data_anomaly.append(sub2[j]) \n",
    "                sub3_data_anomaly.append(sub3[j])\n",
    "                \n",
    "    return glp_data_anomaly,sub1_data_anomaly,sub2_data_anomaly,sub3_data_anomaly\n",
    "\n",
    "def goScatter(showlegend,y):\n",
    "     return go.Scatter(name=\"Anomaly\",\n",
    "                               showlegend=showlegend,\n",
    "                               x=anomaly_date,\n",
    "                               y=y,\n",
    "                               hovertext = [\"Score: \"+str(p) for p in pred_list],\n",
    "                       #       text = [\"Score: \"+str(p) for p in pred_list],\n",
    "                       #       hoverinfo = 'text',\n",
    "                               mode='markers',\n",
    "                               marker=dict(color=\"orange\",\n",
    "                                           size=11,\n",
    "                                           line=dict(\n",
    "                                               color=\"red\",\n",
    "                                               width=2)))\n",
    "\n",
    "def live_plotters1(x_vec,anomalyScores_list,y2_data,y3_data,y4_data,y5_data,y6_data,anomaly_date,pred_list,pause_time=0.1):\n",
    "\n",
    "    plot_data2=go.Scatter(name='glp',showlegend=True,x=x_vec, y=y2_data,line=dict(color='firebrick', width=4))\n",
    "    plot_data4=go.Scatter(name='kitchen',showlegend=True,x=x_vec, y=y4_data)\n",
    "    plot_data5=go.Scatter(name='laundry',showlegend=True,x=x_vec, y=y5_data)\n",
    "    plot_data6=go.Scatter(name='AC',showlegend=True,x=x_vec, y=y6_data)\n",
    "    \n",
    "    y2_data_anomaly,y4_data_anomaly,y5_data_anomaly,y6_data_anomaly = anomaly_map(anomaly_date,y2_data,y4_data,y5_data,y6_data,len(x_vec))       \n",
    "    \n",
    "    anomalies_map2 = goScatter(showlegend=True,y=y2_data_anomaly)\n",
    "    anomalies_map4 = goScatter(showlegend=False,y=y4_data_anomaly)\n",
    "    anomalies_map5 = goScatter(showlegend=False,y=y5_data_anomaly)\n",
    "    anomalies_map6 = goScatter(showlegend=False,y=y6_data_anomaly)\n",
    "    \n",
    "    fig=go.Figure(data = [plot_data2,\n",
    "                        plot_data4,plot_data5,plot_data6,\n",
    "                        anomalies_map2\n",
    "                        ,anomalies_map4,anomalies_map5,anomalies_map6\n",
    "                       ])\n",
    "\n",
    "    \n",
    "    fig.update_layout(title='Housing Power Consumption',\n",
    "                   xaxis_title='Date',\n",
    "                   yaxis_title='Power',\n",
    "                   plot_bgcolor='white')\n",
    "#     print(anomalyScores_list)\n",
    "#     print(pred_list)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_db(spark,current): \n",
    "    # spark = SparkSession.builder.master(\"local[*]\")             .appName(\"IForestExample\")             .config(\"spark.jars\", \"/dbr1/hctl/spark_essentials/mysql-connector-java-5.1.49.jar,/dbr1/hctl/spark_essentials/spark-iforest-2.4.0.jar\").getOrCreate()\n",
    "    # spark = SparkSession.builder.master(\"local[*]\").config(\"spark.jars\", \"/dbr1/hctl/spark_essentials/mysql-connector-java-5.1.49.jar,/dbr1/hctl/spark_essentials/spark-iforest-2.4.0.jar\").getOrCreate()\n",
    "   \n",
    "    sqlContext = SQLContext(spark.sparkContext)\n",
    "    #print(\"Spark Version - \", spark.version)\n",
    "    hostname = \"dbclass.cs.nmsu.edu\" \n",
    "    dbname = \"nmsu_power\"\n",
    "    jdbcPort = 3306\n",
    "    username = \"sakumar\"\n",
    "    password = \"dbnmsu123\"\n",
    "    jdbc_url = \"jdbc:mysql://{0}:{1}/{2}?user={3}&password={4}\".format(hostname,jdbcPort, dbname,username,password)\n",
    "    query = \"\"\"\n",
    "    ( SELECT * FROM power where ID >\"\"\"+str(current)+\"\"\" and ID <=\"\"\"+str(current+50)+\"\"\" ) whyalias\n",
    "    \"\"\"\n",
    "    #print(query)\n",
    "    df_origin = sqlContext.read.format('jdbc').options(driver='com.mysql.jdbc.Driver',url=jdbc_url, dbtable=query ).load()\n",
    "    return df_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_list(iterate_value):\n",
    "    iterate_list = []\n",
    "    for iv in iterate_value:\n",
    "      row_list=[]\n",
    "      for i in iv:\n",
    "       if type(i) == Decimal:\n",
    "        row_list.append(float(i))\n",
    "       elif type(i) == float:\n",
    "        row_list.append(float(i))\n",
    "       elif type(i) == datetime:\n",
    "        row_list.append(i.time())\n",
    "       else:\n",
    "        row_list.append(i)\n",
    "      iterate_list.append(row_list)\n",
    "    return iterate_list\n",
    "\n",
    "\n",
    "def Decimal_append(g):\n",
    "    a=[]\n",
    "    for row in g:\n",
    "        row_data = []\n",
    "        for data in row:\n",
    "            if type(data) == Decimal:\n",
    "                row_data.append(float(data))\n",
    "            elif type(data) == float:\n",
    "                row_data.append(float(data))\n",
    "        a.append(row_data)\n",
    "    return a\n",
    "\n",
    "def iforest_spark(data):\n",
    "##########################\n",
    "\n",
    "# Spark-iForest  \n",
    "\n",
    "##########################\n",
    "\n",
    "# NOTE: features need to be dense vectors for the model input\n",
    "\n",
    "    df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "    # Init an IForest Object\n",
    "    iforest = IForest(contamination=0.1,maxDepth=4)\n",
    "    #print(\"init iforest\")\n",
    "    # Fit on a given data frame\n",
    "    model = iforest.fit(df)\n",
    "    #print(\"fit iforest\")\n",
    "    # Check if the model has summary or not, the newly trained model has the summary info\n",
    "    model.hasSummary\n",
    "\n",
    "    # Show model summary\n",
    "    summary = model.summary\n",
    "\n",
    "    # Show the number of anomalies\n",
    "    summary.numAnomalies\n",
    "\n",
    "    # Predict for a new data frame based on the fitted model\n",
    "    transformed = model.transform(df)\n",
    "\n",
    "    # Collect spark data frame into local df\n",
    "    rows = transformed.collect()\n",
    "\n",
    "#     temp_path = tempfile.mkdtemp()\n",
    "    \n",
    "    temp_path = '/home/grad16/sakumar/sparkNMSU/tmp/'\n",
    "  \n",
    "    shutil.rmtree(temp_path)\n",
    "    os.mkdir(temp_path)\n",
    "    \n",
    "    iforest_path = temp_path + \"/iforest\"\n",
    "    \n",
    "    # Save the iforest estimator into the path\n",
    "    iforest.save(iforest_path)\n",
    "\n",
    "    # Load iforest estimator from a path\n",
    "    loaded_iforest = IForest.load(iforest_path)\n",
    "\n",
    "    model_path = temp_path + \"/iforest_model\"\n",
    "\n",
    "    # Save the fitted model into the model path\n",
    "    model.save(model_path)\n",
    "\n",
    "    # Load a fitted model from a model path\n",
    "    loaded_model = IForestModel.load(model_path)\n",
    "\n",
    "    # The loaded model has no summary info\n",
    "    loaded_model.hasSummary\n",
    "\n",
    "    # Use the loaded model to predict a new data frame\n",
    "    new_df=loaded_model.transform(df)\n",
    "#     new_df.show()\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iforest_run(current):\n",
    "    \n",
    "    df_origin = read_db(spark,current)\n",
    "#     df_origin.show() \n",
    "    df_origin = df_origin.na.drop()\n",
    "#     print((df_origin.count(), len(df_origin.columns)))\n",
    "###### Data Conversion ########\n",
    "\n",
    "    date=df_origin.select(to_date(df_origin['Date'], 'dd/MM/yyyy').alias('date')).collect()\n",
    "    df_time=df_origin.select(to_timestamp(df_origin['Time'], 'yyyy-MM-dd HH:mm:ss').alias('time')).collect()\n",
    "    date_list= df_to_list(date)\n",
    "    time_list= df_to_list(df_time)\n",
    "\n",
    "    date_time_list=[]\n",
    "    for i in range(len(time_list)):\n",
    "      date_time_list.append(datetime.combine(date_list[i][0],time_list[i][0]))\n",
    "    \n",
    "    columns_to_drop=[\"Date\",\"Time\"]\n",
    "    df1=df_origin.drop(*columns_to_drop)\n",
    "\n",
    "    a=[column for column in df1.columns]\n",
    "    g=df1.select(a).collect()\n",
    "    size=0\n",
    "    a=Decimal_append(g)\n",
    "    \n",
    "    # a=a[-last:]\n",
    "    a=[[i / sum(j) for i in j] for j in a]\n",
    "    data = [(Vectors.dense(i),) for i in a]\n",
    "    \n",
    "    new_df = iforest_spark(data)\n",
    "    \n",
    "    columns_to_drop=[\"Date\",\"Time\"]\n",
    "    df1=df_origin.drop(*columns_to_drop)\n",
    "    \n",
    "    anomalyScores_list = [row['anomalyScore'] for row in new_df.toLocalIterator()]\n",
    "    plist= [row['prediction'] for row in new_df.toLocalIterator()]\n",
    "\n",
    "    anomaly_date=[]\n",
    "    for i,j in zip(plist,date_time_list):\n",
    "      if(i==1.0):\n",
    "        anomaly_date.append(j)\n",
    "\n",
    "    pred_1=new_df.filter(new_df['prediction']==1.0).select(new_df['anomalyScore']).collect()  \n",
    "    pred_list= df_to_list(pred_1)    \n",
    "    pred_list = [item for sublist in pred_list for item in sublist]\n",
    "    \n",
    "    glp = [float(row['global_active_power']) for row in df_origin.toLocalIterator()]\n",
    "    grp = [float(row['global_reactive_power']) for row in df_origin.toLocalIterator()]\n",
    "    sub1= [row['sub_metering_1'] for row in df_origin.toLocalIterator()]\n",
    "    sub2= [row['sub_metering_2'] for row in df_origin.toLocalIterator()]\n",
    "    sub3= [float(row['sub_metering_3']) for row in df_origin.toLocalIterator()]\n",
    "    vol = [float(row['voltage']) for row in df_origin.toLocalIterator()]\n",
    "    gi = [float(row['global_intensity']) for row in df_origin.toLocalIterator()]\n",
    "    \n",
    "    return(date_time_list,glp, grp,sub1,sub2,sub3,anomalyScores_list,anomaly_date,pred_list,vol,gi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fee617e51000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mx_vec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msub1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msub2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msub3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manomalyScores_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manomaly_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miforest_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0my_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_vec2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_vec3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_vec4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_vec5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_vec6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manomalyScores_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msub1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msub2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msub3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-93e726f4cca5>\u001b[0m in \u001b[0;36miforest_run\u001b[0;34m(current)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miforest_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mcolumns_to_drop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Date\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Time\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-783f2378c3eb>\u001b[0m in \u001b[0;36miforest_spark\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Predict for a new data frame based on the fitted model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mtransformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Collect spark data frame into local df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from IPython import display\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#current = 9656\n",
    "current = 1\n",
    "\n",
    "line1 , line2, line3, line4, line5, line6= [[] for _ in range(6)]\n",
    "\n",
    "while True:\n",
    "    x_vec,glp, grp,sub1,sub2,sub3,anomalyScores_list,anomaly_date,pred_list,vol,gi=iforest_run(current) \n",
    "    y_vec, y_vec2, y_vec3,y_vec4,y_vec5,y_vec6 = anomalyScores_list,glp, grp,sub1,sub2,sub3\n",
    "    \n",
    "#     line1 = live_plotter(x_vec,y_vec,line1)\n",
    "#     line2,line3,line4,line5,line6 = live_plotters(x_vec,y_vec,y_vec2,y_vec3,y_vec4,y_vec5,y_vec6,line1,line2,line3,line4,line5,line6,anomaly_date,pred_list)\n",
    "    \n",
    "    fig = live_plotters1(x_vec,y_vec,y_vec2,y_vec3,y_vec4,y_vec5,y_vec6,anomaly_date,pred_list)\n",
    "#     ax.set_xlim(left=date_time_list[i], right=date_time_list[i+5])\n",
    "#    plt.gcf().canvas.draw()\n",
    "#     display.display(plt.gcf())\n",
    "\n",
    "    display.display(fig)\n",
    "    display.clear_output(wait=True)\n",
    "    current = current + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "def goScatter(showlegend,y):\n",
    "    \n",
    "     return go.Scatter(name=\"Anomaly\",\n",
    "                               showlegend=showlegend,\n",
    "                               x=anomaly_date,\n",
    "                               y=y,\n",
    "                               hovertext = [\"Score: \"+str(p) for p in pred_list],\n",
    "                       #       text = [\"Score: \"+str(p) for p in pred_list],\n",
    "                       #       hoverinfo = 'text',\n",
    "                               mode='markers',\n",
    "                               marker=dict(color=\"orange\",\n",
    "                                           size=11,\n",
    "                                           line=dict(\n",
    "                                               color=\"red\",\n",
    "                                               width=2)))\n",
    "\n",
    "\n",
    "def anomaly_map(anomaly_date,glp,sub1,sub2,sub3,size):\n",
    "    glp_data_anomaly,sub1_data_anomaly,sub2_data_anomaly,sub3_data_anomaly=[[] for _ in range(4)]\n",
    "    for i in anomaly_date:\n",
    "        for j in range(size):\n",
    "            if(i==x_vec[j]):\n",
    "                glp_data_anomaly.append(glp[j])\n",
    "                sub1_data_anomaly.append(sub1[j]) \n",
    "                sub2_data_anomaly.append(sub2[j]) \n",
    "                sub3_data_anomaly.append(sub3[j])\n",
    "                \n",
    "    return glp_data_anomaly,sub1_data_anomaly,sub2_data_anomaly,sub3_data_anomaly\n",
    "\n",
    "\n",
    "#current = 9656\n",
    "current = 19960\n",
    "max_ID= 19965\n",
    "\n",
    "frames = []\n",
    "\n",
    "while current < max_ID:\n",
    "    x_vec,glp,grp,sub1,sub2,sub3,anomalyScores_list,anomaly_date,pred_list,vol,gi=iforest_run(current) \n",
    "    print(current)\n",
    "    plot_glp = go.Scatter(name='glp',showlegend=True,x=x_vec, y=glp,line=dict(color='firebrick', width=4))\n",
    "\n",
    "#     plot_sub1 = go.Scatter(name='kitchen',showlegend=True,x=x_vec, y=sub1)\n",
    "#     plot_sub2 = go.Scatter(name='laundry',showlegend=True,x=x_vec, y=sub2)\n",
    "#     plot_sub3 = go.Scatter(name='AC',showlegend=True,x=x_vec, y=sub3)\n",
    "    \n",
    "#     glp_data_anomaly,sub1_data_anomaly,sub2_data_anomaly,sub3_data_anomaly=[[] for _ in range(4)]\n",
    "#     glp_data_anomaly,sub1_data_anomaly,sub2_data_anomaly,sub3_data_anomaly = anomaly_map(anomaly_date,glp,sub1,sub2,sub3,len(x_vec))\n",
    "    \n",
    "#     anomalies_glp = goScatter(showlegend=True,y=glp_data_anomaly)\n",
    "#     anomalies_sub1 = goScatter(showlegend=False,y=sub1_data_anomaly)\n",
    "#     anomalies_sub2 = goScatter(showlegend=False,y=sub2_data_anomaly)\n",
    "#     anomalies_sub3 = goScatter(showlegend=False,y=sub3_data_anomaly)\n",
    "    \n",
    "    current_frame = go.Frame(data = go.Scatter(name='glp',showlegend=True,x=x_vec, y=glp,line=dict(color='firebrick', width=4)))\n",
    "#                                          ,plot_sub1,plot_sub2,plot_sub3,anomalies_glp,anomalies_sub1,anomalies_sub2,anomalies_sub3\n",
    "                                        \n",
    "    frames.append(current_frame)\n",
    "    current= current+1\n",
    "\n",
    "fig = go.Figure(layout=go.Layout(\n",
    "                         updatemenus=[dict(\n",
    "                                type=\"buttons\",\n",
    "                                buttons=[dict(label=\"Play\",\n",
    "                          method=\"animate\",\n",
    "                          args=[None])])]\n",
    "    ),\n",
    "                frames=frames\n",
    "                )\n",
    "display.display(fig)\n",
    "display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib notebook\n",
    "\n",
    "# plt.rcParams['animation.html'] = 'jshtml'\n",
    "\n",
    "fig = plt.figure(figsize=(13,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_ylabel('Power Consumption')\n",
    "ax.set_xlabel('Timestamp')\n",
    "# ax.legend(handles=['Score'])\n",
    "# ax.legend()\n",
    "fig.show()\n",
    "\n",
    "current = 9656\n",
    "\n",
    "x_vec,glp, grp,sub1,sub2,sub3,anomalyScores_list,anomaly_date,pred_list,vol,gi=iforest_run(current)\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "d = 0\n",
    "\n",
    "x, y =[] , []\n",
    "\n",
    "# s1,s2,s3 = [], [] , [] , []\n",
    "\n",
    "# print(date_time_list[2])\n",
    "\n",
    "while True:\n",
    "    x.append(x_vec[i])\n",
    "    y.append(anomalyScores_list[i])\n",
    "#     s1.append(sub1[i])\n",
    "#     s2.append(sub2[i])\n",
    "#     s3.append(sub3[i])\n",
    "#     ax.plot(x, glp, color='b',label='Score')\n",
    "    ax.plot(x,y, color='b')\n",
    "#     ax.plot(x,s2,color='g')\n",
    "#     ax.plot(x,s3,color='r')\n",
    "    \n",
    "    if(anomaly_date[d] == x_vec[i]):\n",
    "        ax.scatter(anomaly_date[d],pred_list[d])\n",
    "        d=d+1\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    ax.set_xlim(left=max(datetime(2006, 12, 21, 4, 6),x_vec[i]), right=x_vec[i+5])\n",
    "#     ax.set_xlim(left=max(0, i-50), right=i+50)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    i = i +  1\n",
    "    j = j + 2\n",
    "    \n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "size = 50\n",
    "x_vec = np.linspace(0,1,size+1)[0:-1]\n",
    "y_vec = np.random.randn(len(x_vec))\n",
    "line1 = []\n",
    "\n",
    "while True:\n",
    "    #iforest\n",
    "    #read last 100 datapoints\n",
    "    #run iforest\n",
    "    #draw\n",
    "    \n",
    "    rand_val = np.random.randn(1)\n",
    "    y_vec[-1] = rand_val\n",
    "    y_vec = np.append(y_vec[1:],0.0)\n",
    "\n",
    "#     line1 = live_plotters(x_vec,y_vec,y_vec2,line1,line2)\n",
    "    line1 = live_plotter(x_vec,y_vec,line1)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython import display\n",
    "# from iforest import iforest_spark\n",
    "\n",
    "size = 50\n",
    "x_vec = np.linspace(0,1,size+1)[0:-1]\n",
    "y_vec = np.random.randn(len(x_vec))\n",
    "line1 = []\n",
    "\n",
    "x_vec2 = np.linspace(0,1,size+1)[0:-1]\n",
    "y_vec2 = 4 * np.random.randn(len(x_vec2))\n",
    "line2 = []\n",
    "\n",
    "# print(y_vec)\n",
    "# print(x_vec)\n",
    "\n",
    "\n",
    "while True:\n",
    "    #iforest\n",
    "    #read last 100 datapoints\n",
    "    #run iforest\n",
    "    #draw\n",
    "    \n",
    "    rand_val = np.random.randn(1)\n",
    "    y_vec[-1] = rand_val\n",
    "    y_vec = np.append(y_vec[1:],0.0)\n",
    "    \n",
    "    rand_val2 = np.random.randn(1)\n",
    "    y_vec2[-1] = 4 * rand_val2\n",
    "    y_vec2 = np.append(y_vec2[1:],0.0)\n",
    "    \n",
    "    line1,line2 = live_plotters(x_vec,y_vec,y_vec2,y_vec3,y_vec4,y_vec5, y_vec6,line1,line2,line3,line4,line5,line6,anomaly_date,pred_list)\n",
    "    \n",
    "#     line1 = live_plotter(x_vec,y_vec,line1)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    time.sleep(0.1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "for i in range(10):\n",
    "    pl.plot(pl.randn(100))\n",
    "    display.display(pl.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

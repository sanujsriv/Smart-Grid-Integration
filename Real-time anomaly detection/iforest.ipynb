{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/dbr1/hctl/spark_essentials/jdk1.8.0_251/\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/local/spark-2.4.0-bin-hadoop2.7/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark_iforest.ml.iforest import *\n",
    "from decimal import Decimal\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import to_timestamp,to_date\n",
    "from datetime import datetime\n",
    "import time as realtime\n",
    "from pyspark.sql import SQLContext,SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iforest_spark(): \n",
    "    spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "            .appName(\"IForestExample\") \\\n",
    "            .config(\"spark.jars\", \"/dbr1/hctl/spark_essentials/mysql-connector-java-5.1.49.jar,/dbr1/hctl/spark_essentials/spark-iforest-2.4.0.jar\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "    # spark = SparkSession.builder.master(\"local[*]\").config(\"spark.jars\", \"/dbr1/hctl/spark_essentials/mysql-connector-java-5.1.49.jar,/dbr1/hctl/spark_essentials/spark-iforest-2.4.0.jar\").getOrCreate()\n",
    "\n",
    "    sqlContext = SQLContext(spark.sparkContext)\n",
    "\n",
    "    print(\"Spark Version - \", spark.version)\n",
    "\n",
    "    hostname = \"dbclass.cs.nmsu.edu\" \n",
    "    dbname = \"nmsu_power\"\n",
    "    jdbcPort = 3306\n",
    "    username = \"sakumar\"\n",
    "    password = \"dbnmsu123\"\n",
    "    jdbc_url = \"jdbc:mysql://{0}:{1}/{2}?user={3}&password={4}\".format(hostname,jdbcPort, dbname,username,password)\n",
    "\n",
    "    query = \"\"\"\n",
    "    ( SELECT * FROM (SELECT * FROM power ORDER BY Date DESC, TIME DESC LIMIT 100 ) sub ORDER BY Date ASC, TIME ASC )\n",
    "    whyalias\n",
    "    \"\"\"\n",
    "\n",
    "    df_origin = sqlContext.read.format('jdbc').options(driver='com.mysql.jdbc.Driver',url=jdbc_url, dbtable=query ).load()\n",
    "    #df_origin.show()\n",
    "    print(\"db connected successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_origin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ff12f6dd51ba>\u001b[0m in \u001b[0;36masync-def-wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_origin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_origin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'yyyy-MM-dd HH:mm:ss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdate_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtime_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdate_time_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_origin' is not defined"
     ]
    }
   ],
   "source": [
    "###### Data Conversion ########\n",
    "\n",
    "    date=df_origin.select(to_date(df_origin['Date'], 'dd/MM/yyyy').alias('date')).collect()\n",
    "    df_time=df_origin.select(to_timestamp(df_origin['Time'], 'yyyy-MM-dd HH:mm:ss').alias('time')).collect()\n",
    "\n",
    "    date_list=[]\n",
    "    time_list=[]\n",
    "    date_time_list=[]\n",
    "\n",
    "    for da in date:\n",
    "      row_list=[]\n",
    "      for d in da:\n",
    "        row_list.append(d)\n",
    "      date_list.append(row_list)\n",
    "\n",
    "    for ti in df_time:\n",
    "      row_list=[]\n",
    "      for t in ti:\n",
    "        row_list.append(t.time())\n",
    "      time_list.append(row_list)\n",
    "\n",
    "    for i in range(len(time_list)):\n",
    "      date_time_list.append(datetime.combine(date_list[i][0],time_list[i][0]))\n",
    "\n",
    "    # last=100\n",
    "    # date_time_list = date_time_list[-last:]\n",
    "\n",
    "    columns_to_drop=[\"Date\",\"Time\"]\n",
    "    df1=df_origin.drop(*columns_to_drop)\n",
    "\n",
    "    a=[column for column in df1.columns]\n",
    "    g=df1.select(a).collect()\n",
    "\n",
    "    # record_count=df1.count()\n",
    "    # print(record_count)\n",
    "\n",
    "    size=0\n",
    "    a=[]\n",
    "    for row in g:\n",
    "        row_data = []\n",
    "        for data in row:\n",
    "            if type(data) == Decimal:\n",
    "                row_data.append(float(data))\n",
    "            elif type(data) == float:\n",
    "                row_data.append(float(data))\n",
    "        a.append(row_data)\n",
    "\n",
    "    # a=a[-last:]\n",
    "    a=[[i / sum(j) for i in j] for j in a]\n",
    "    data = [(Vectors.dense(i),) for i in a]\n",
    "\n",
    "\n",
    "\n",
    "    print(\"converted data successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init iforest\n",
      "fit iforest\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "\n",
    "# Spark-iForest  \n",
    "\n",
    "##########################\n",
    "\n",
    "# NOTE: features need to be dense vectors for the model input\n",
    "\n",
    "    df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "    # Init an IForest Object\n",
    "    iforest = IForest(contamination=0.1,maxDepth=4)\n",
    "    print(\"init iforest\")\n",
    "    # Fit on a given data frame\n",
    "    model = iforest.fit(df)\n",
    "    print(\"fit iforest\")\n",
    "    # Check if the model has summary or not, the newly trained model has the summary info\n",
    "    model.hasSummary\n",
    "\n",
    "    # Show model summary\n",
    "    summary = model.summary\n",
    "\n",
    "    # Show the number of anomalies\n",
    "    summary.numAnomalies\n",
    "\n",
    "    # Predict for a new data frame based on the fitted model\n",
    "    transformed = model.transform(df)\n",
    "\n",
    "    # Collect spark data frame into local df\n",
    "    rows = transformed.collect()\n",
    "\n",
    "    temp_path = tempfile.mkdtemp()\n",
    "    iforest_path = temp_path + \"/iforest\"\n",
    "\n",
    "    # Save the iforest estimator into the path\n",
    "    iforest.save(iforest_path)\n",
    "\n",
    "    # Load iforest estimator from a path\n",
    "    loaded_iforest = IForest.load(iforest_path)\n",
    "\n",
    "    model_path = temp_path + \"/iforest_model\"\n",
    "\n",
    "    # Save the fitted model into the model path\n",
    "    model.save(model_path)\n",
    "\n",
    "    # Load a fitted model from a model path\n",
    "    loaded_model = IForestModel.load(model_path)\n",
    "\n",
    "    # The loaded model has no summary info\n",
    "    loaded_model.hasSummary\n",
    "\n",
    "    # Use the loaded model to predict a new data frame\n",
    "    new_df=loaded_model.transform(df)\n",
    "#     new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_origin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7632704a2374>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcolumns_to_drop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Date\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Time\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_origin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcolumns_to_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0manomalyScores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'anomalyScore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_origin' is not defined"
     ]
    }
   ],
   "source": [
    "    columns_to_drop=[\"Date\",\"Time\"]\n",
    "    df1=df_origin.drop(*columns_to_drop)\n",
    "\n",
    "    predictions=new_df.select('prediction').collect()\n",
    "    anomalyScores = new_df.select('anomalyScore').collect()\n",
    "\n",
    "    anomalyScores_list=[]\n",
    "    for an in anomalyScores:\n",
    "      row_list=[]\n",
    "      for a in an:\n",
    "        row_list.append(a)\n",
    "      anomalyScores_list.append(row_list)\n",
    "    anomalyScores_list = [item for sublist in anomalyScores_list for item in sublist]\n",
    "\n",
    "\n",
    "    plist=[]\n",
    "    for an in predictions:\n",
    "      row_list=[]\n",
    "      for a in an:\n",
    "        row_list.append(a)\n",
    "      plist.append(row_list)\n",
    "    plist = [item for sublist in plist for item in sublist]\n",
    "\n",
    "    # print(plist)\n",
    "\n",
    "    # print(date_time_list)\n",
    "    # print(anomalyScores_list)\n",
    "\n",
    "    anomaly_date=[]\n",
    "    for i,j in zip(plist,date_time_list):\n",
    "      if(i==1.0):\n",
    "        anomaly_date.append(j)\n",
    "\n",
    "    # print(anomaly_date)\n",
    "\n",
    "    pred_1=new_df.filter(new_df['prediction']==1.0).select(new_df['anomalyScore']).collect()\n",
    "\n",
    "    pred_list=[]\n",
    "    for an in pred_1:\n",
    "      row_list=[]\n",
    "      for a in an:\n",
    "        row_list.append(a)\n",
    "      pred_list.append(row_list)\n",
    "    pred_list = [item for sublist in pred_list for item in sublist]\n",
    "\n",
    "    # print(pred_list)\n",
    "    # print(anomalyScores_list)\n",
    "\n",
    "    print(\"Percentage of anomalies in data: {:.2f}\".format( (len(pred_list) / len(plist) )*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Histogram ########\n",
    "\n",
    "\n",
    "#     pandas_df=new_df.toPandas()\n",
    "\n",
    "#     pandas_df.hist(column='anomalyScore')\n",
    "#     pandas_df.hist(column='prediction')\n",
    "\n",
    "#     print(pred_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ### Plots ########\n",
    "#     import numpy as np\n",
    "#     from sklearn.preprocessing import normalize\n",
    "#     from datetime import datetime\n",
    "#     from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "#     # import matplotlib.pyplot as plt\n",
    "#     # from matplotlib import pyplot\n",
    "#     import plotly.graph_objects as go\n",
    "\n",
    "#     plot_data=go.Scatter(name='Data',showlegend=True,x=date_time_list, y=anomalyScores_list)\n",
    "\n",
    "#     anomalies_map = go.Scatter(name=\"Anomaly\",\n",
    "#                                    showlegend=True,\n",
    "#                                    x=anomaly_date,\n",
    "#                                    y=pred_list,\n",
    "#                                    mode='markers',\n",
    "#                                    marker=dict(color=\"red\",\n",
    "#                                                size=11,\n",
    "#                                                line=dict(\n",
    "#                                                    color=\"red\",\n",
    "#                                                        width=2)))\n",
    "\n",
    "#     # norm_1=normalize([[row['sub_metering_1'] for row in df_origin.toLocalIterator()]])\n",
    "#     # norm_2=normalize([[row['sub_metering_2'] for row in df_origin.toLocalIterator()]])\n",
    "#     # norm_3=normalize([[float(row['sub_metering_3']) for row in df_origin.toLocalIterator()]])\n",
    "\n",
    "#     plot_data2=go.Scatter(name='sub1',showlegend=True,x=date_time_list,  marker=dict(color=\"green\"),\n",
    "#                           y=[row['sub_metering_1'] for row in df_origin.toLocalIterator()])\n",
    "\n",
    "#     plot_data3=go.Scatter(name='sub2',showlegend=True,x=date_time_list,  marker=dict(color=\"goldenrod\"),\n",
    "#                           y=[row['sub_metering_2'] for row in df_origin.toLocalIterator()])\n",
    "#     plot_data4=go.Scatter(name='sub3',showlegend=True,x=date_time_list,  marker=dict(color=\"black\"),\n",
    "#                           y=[float(row['sub_metering_3']) for row in df_origin.toLocalIterator()])\n",
    "\n",
    "\n",
    "\n",
    "#     fig=go.Figure(data=[plot_data,\n",
    "#                         plot_data2,\n",
    "#                         plot_data3,\n",
    "#                         plot_data4,\n",
    "#                         anomalies_map\n",
    "#                        ])\n",
    "\n",
    "\n",
    "    # fig = go.FigureWidget()\n",
    "    # fig.add_scatter()\n",
    "    # fig\n",
    "\n",
    "    # plot_name='anomaly_'+str(datetime.now())\n",
    "    # fig.write_html(plot_name)\n",
    "\n",
    "    # print('Output Saved! as '+plot_name+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # for i in range(len(anomalyScores_list)):\n",
    "    #     realtime.sleep(0.3)\n",
    "    #     with fig.batch_update():\n",
    "    #         fig.data[0].y = anomalyScores_list[:i]\n",
    "#     fig.show()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pylive import live_plotter\n",
    "# import numpy as np\n",
    "\n",
    "# size = 100\n",
    "# x_vec = np.linspace(0,1,size+1)[0:-1]\n",
    "# y_vec = np.random.randn(len(x_vec))\n",
    "# line1 = []\n",
    "# while True:\n",
    "#     rand_val = np.random.randn(1)\n",
    "#     y_vec[-1] = rand_val\n",
    "#     line1 = live_plotter(x_vec,y_vec,line1)\n",
    "#     y_vec = np.append(y_vec[1:],0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-70-a30677f0cc8e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-70-a30677f0cc8e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    return(anomalyScoreList,predList)\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "    return(anomalyScoreList,predList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
